# -*- coding: utf-8 -*-
"""Llama_finetune_

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1if4PPIuVjXxCOsKUpD1Tb1n76FVwM8HH

**Fine tuning Llama 7B with rhe Corefl dataset**
"""

!pip install accelerate
!pip install bitsandbytes

!pip install ludwig
!pip install ludwig[llm]

#to confirm accelerate and bitsandbytes are installed:
!pip show accelerate
!pip show bitsandbytes

"""This section below creates an environment variable to set the HuggingFace api key"""

import getpass
import locale
locale.getpreferredencoding = lambda: "UTF-8"
import logging
import os
import torch
import yaml

from ludwig.api import LudwigModel


os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")
assert os.environ["HUGGING_FACE_HUB_TOKEN"]

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Corefl.csv', sep='\t')
#only extract proficiency and text (sample produced by user) from the dataframe
df = df = df[['Proficiency', 'Text']]

import numpy as np; np.random.seed(123)

#create a new column called `split` where:
#90% will be assigned a value of 0 -> train set
#5% will be assigned a value of 1 -> validation set
#5% will be assigned a value of 2 -> test set
#calculate the number of rows for each split value
total_rows = len(df)
print(total_rows)
split_0_count = int(total_rows * 0.9)
split_1_count = int(total_rows * 0.05)
split_2_count = total_rows - split_0_count - split_1_count

#Create an array with split values based on the counts
split_values = np.concatenate([
    np.zeros(split_0_count),
    np.ones(split_1_count),
    np.full(split_2_count, 2)
])
print(split_values)

#Shuffle the array to ensure randomness
np.random.shuffle(split_values)

#Add the 'split' column to the DataFrame
df['split'] = split_values
df['split'] = df['split'].astype(int)

#for now, only limit the work to the first 100 rows of the dataset
df = df.head(n=3)
print(df)

print(df.memory_usage(deep=True).sum())

from google.colab import data_table; data_table.enable_dataframe_formatter()

df2 = pd.read_json("https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_20k.json")

# We're going to create a new column called `split` where:
# 90% will be assigned a value of 0 -> train set
# 5% will be assigned a value of 1 -> validation set
# 5% will be assigned a value of 2 -> test set
# Calculate the number of rows for each split value
total_rows = len(df2)
split_0_count = int(total_rows * 0.9)
split_1_count = int(total_rows * 0.05)
split_2_count = total_rows - split_0_count - split_1_count

# Create an array with split values based on the counts
split_values = np.concatenate([
    np.zeros(split_0_count),
    np.ones(split_1_count),
    np.full(split_2_count, 2)
])

# Shuffle the array to ensure randomness
np.random.shuffle(split_values)

# Add the 'split' column to the DataFrame
df2['split'] = split_values
df2['split'] = df2['split'].astype(int)

# For this webinar, we will just 100 rows of this dataset.
df2 = df2.head(n=100)
print(df2.memory_usage(deep=True).sum())

"""The maximum context length for the base LLaMA-2 model is 4096 tokens. This section will test some metrics about the number of tokens in our dataset. While the Proficiency column should be good, the Text column might be problematic because of lengthy inputs."""

#calculating the length of each cell in each column
df['num_characters_Text'] = df['Text'].apply(lambda x: len(x))
df['num_characters_Proficiency'] = df['Proficiency'].apply(lambda x: len(x))


#show Distribution
df.hist(column=['num_characters_Proficiency', 'num_characters_Text'])

#calculate the average number of charachters in the "Text" column
text_column_mean = df["num_characters_Text"].mean()
print(f"the average number of charachters in Text column is: {text_column_mean}.\n")

"""Now that token limits are met, let's move on to using Ludwig to analyze Llama's performance on predicting learner levels as an off-the-shelf model without any fine-tuning."""

zero_shot_config = yaml.safe_load(
  """
  model_type: llm
  base_model: meta-llama/Llama-2-7b-hf

  input_features:
    - name: Text
      type: text

  output_features:
    - name: Proficiency
      type: text

  prompt:
    template: >-
      Below is a paragraph produced by an English language learner. Assess the
      complexity of the paragraph and predict the user's proficiency level.
      The accepted proficiency levels from lowest to highest are A1, A2, B1, B2, C1, and C2.

      ### Paragraph: {Text}

      ### Proficiency level:

  generation:
    temperature: 0.1 # Temperature is used to control the randomness of predictions.
    max_new_tokens: 512

  preprocessing:
    split:
      type: fixed

  quantization:
    bits: 4
  """
)

# Just run on 10 examples for now
model = LudwigModel(config=zero_shot_config, logging_level=logging.INFO)
results = model.train(dataset=df)

def clear_cache():
  if torch.cuda.is_available():
    model = None
    torch.cuda.empty_cache()

clear_cache()

model = None
clear_cache()

qlora_fine_tuning_config = yaml.safe_load(
"""
model_type: llm
base_model: meta-llama/Llama-2-7b-hf

input_features:
    - name: Text
      type: text

output_features:
    - name: Proficiency
      type: text

prompt:
  template: >-
    Below is a paragraph produced by an English language learner. Assess the
    complexity of the paragraph and predict the user's proficiency level.
    The accepted proficiency levels from lowest to highest are A1, A2, B1, B2, C1, and C2.

    ### Paragraph: {Text}

    ### Proficiency level:

generation:
  temperature: 0.1
  max_new_tokens: 512

adapter:
  type: lora

quantization:
  bits: 4

trainer:
  type: finetune
  epochs: 5
  batch_size: 1
  eval_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 0.00001
  optimizer:
    type: adam
    params:
      eps: 1.e-8
      betas:
        - 0.9
        - 0.999
      weight_decay: 0
  learning_rate_scheduler:
    warmup_fraction: 0.03
    reduce_on_plateau: 0
"""
)

model = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)
results = model.train(dataset=df)